# Sistema RAG (Retrieval-Augmented Generation) com Ollama

Sistema de perguntas e respostas baseado em documentos PDF usando embeddings locais e modelos de linguagem do Ollama.

## VisÃ£o Geral

Este projeto implementa um sistema RAG (Retrieval-Augmented Generation) que permite fazer perguntas sobre documentos PDF. O sistema:

1. Carrega e processa PDFs da pasta `base/`
2. Divide os documentos em chunks
3. Gera embeddings usando o modelo `mxbai-embed-large` do ollama
4. Armazena os vetores no banco Chroma
5. Responde perguntas buscando contexto relevante nos documentos disponibilizados

---

### Softwares Utilizados

- **Python 3.12+**
- **Ollama** instalado e rodando ([Download](https://ollama.com/download))

### Modelos do Ollama

#### Modelo de embeddings (1024 dimensÃµes)
- mxbai-embed-large

#### Modelo de chat
- llama3.2

**Obs: O Ollama precisa estar rodando esses dois modelos**

### Bibliotecas Python utilizadas

```bash
pip install python-dotenv langchain langchain-core langchain-ollama \
            langchain-community langchain-chroma chromadb ollama pypdf
```

---

## Estrutura do Projeto

```
projeto/
â”œâ”€â”€ base/                    # ğŸ“ PDFs de entrada
â”‚   â”œâ”€â”€ documento1.pdf
â”‚   â””â”€â”€ documento2.pdf
â”œâ”€â”€ db/                      # Banco vetorial Chroma (gerado)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ criar_db.ipynb           # Script para criar o banco
â”œâ”€â”€ main.ipynb               # Script principal (perguntas)
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente (se precisar)
â””â”€â”€ README.md                # ğŸ“– README
```

---

## Como funciona?

### Adicione seus PDFs

Na pasta `base/` e coloque seus arquivos PDF:

```
projeto/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ documento1.pdf
â”‚   â”œâ”€â”€ documento2.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ criar_db.ipynb
â””â”€â”€ main.ipynb
```

---

### Crie o Banco de Dados

Execute o notebook `criar_db.ipynb`:

```python
# Executa todas as cÃ©lulas do criar_db.ipynb
# Isso irÃ¡:
# 1. Carregar PDFs da pasta base/
# 2. Dividir em chunks de 2000 caracteres
# 3. Gerar embeddings com mxbai-embed-large
# 4. Salvar no banco Chroma (db/)
```

**SaÃ­da esperada:** <br>
Na Ãºltima cÃ©lula vocÃª verÃ¡ algo como
`Banco de dados Criado`
se o banco tiver sido criado


### FaÃ§a suas Perguntas

Execute o notebook `main.ipynb`:

```python
# Executa a funÃ§Ã£o perguntar()
# Digite sua pergunta e pressione Enter
```
## ConclusÃ£o

Este sistema RAG oferece uma soluÃ§Ã£o completa e 100% local para consulta inteligente de documentos PDF, sem depender de APIs externas pagas. Ao combinar o poder dos embeddings do Ollama com o banco vetorial Chroma

---

## ğŸ”— Links Ãšteis

- [Ollama](https://ollama.com/)
- [LangChain Docs](https://python.langchain.com/)
- [Chroma DB](https://www.trychroma.com/)
- [Modelos Ollama](https://ollama.com/library)

---
